{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "818d325e",
   "metadata": {
    "id": "zYEJnedmLpR2",
    "papermill": {
     "duration": 0.002852,
     "end_time": "2025-11-03T23:33:05.555011",
     "exception": false,
     "start_time": "2025-11-03T23:33:05.552159",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Best case optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e4f7b6",
   "metadata": {
    "id": "e7PxOjcjLpR7",
    "papermill": {
     "duration": 0.0019,
     "end_time": "2025-11-03T23:33:05.559301",
     "exception": false,
     "start_time": "2025-11-03T23:33:05.557401",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### In general\n",
    "- **goal:** try to predict the theoretical limit of embedding dimension\n",
    "- best case: try to optimize both query and document embeddings directly $\\implies$ no limit of the natural language $\\implies$ free embeddings\n",
    "- if there are limits for this best case, then there are limits for any real world scenario\n",
    "- the qrel matrix is given previously"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018bbe4e",
   "metadata": {
    "id": "Er9mN19yLpR8",
    "papermill": {
     "duration": 0.001925,
     "end_time": "2025-11-03T23:33:05.563396",
     "exception": false,
     "start_time": "2025-11-03T23:33:05.561471",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Experimental settings\n",
    "- training set:\n",
    "    - $n$ document embeddings with dimension $d$ - $d$ is controlled\n",
    "    - $m$ query vectors with dimension $d$\n",
    "    - only mark top-$k$ relevant documents for each query as relevant in the qrel matrix\n",
    "    - we aim to have max number of different query results $\\implies m:= \\binom{n}{k}$\n",
    "    - 2 queries have same top-$k$ results $\\implies$ $+k(n-k)$ similar inequalities $\\implies$ no new indepent constrains $\\implies$ duplicated row in $2A - 1_{m\\times n} \\iff $ duplicated rows in $A$\n",
    "    - we construct $A$ by enumerating all possible cases\n",
    "- loss function - based on InfoNCE (but engineered without the negative event):\n",
    "      $L_\\text{total}=\\displaystyle-\\frac{1}{M}\\sum_{i=1}^M\\log\\frac{\\sum_{d_r\\in R_i}\\exp(\\text{sim}(q_i, d_r)/\\tau)}{\\sum_{d_k\\in D}\\exp(\\text{sim}(q_i, d_k)/\\tau)}$ where\n",
    "    - $R_i = \\{d_r \\in D \\mid d_r \\text{ relevan to query } q_i\\}$\n",
    "    - $D$ set of documents\n",
    "    - $k:=2$ - for simplicity\n",
    "- optimizer: SGD and Adam, but Adam was mainly used for its speed\n",
    "- normalization after each update\n",
    "- early stopping if no better result (in the next $1000$ iterations)\n",
    "- for fixed $d$ increase the $n$ num of docs, until the optimization model can't reach $100\\%$ accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e1fb592",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:33:05.568805Z",
     "iopub.status.busy": "2025-11-03T23:33:05.568217Z",
     "iopub.status.idle": "2025-11-03T23:33:05.734236Z",
     "shell.execute_reply": "2025-11-03T23:33:05.733533Z"
    },
    "id": "UXzIDSjgLpR-",
    "papermill": {
     "duration": 0.169904,
     "end_time": "2025-11-03T23:33:05.735420",
     "exception": false,
     "start_time": "2025-11-03T23:33:05.565516",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov  3 23:33:05 2025       \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\r\n",
      "|-----------------------------------------+------------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                        |               MIG M. |\r\n",
      "|=========================================+========================+======================|\r\n",
      "|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\r\n",
      "| N/A   35C    P0             26W /  250W |       0MiB /  16384MiB |      0%      Default |\r\n",
      "|                                         |                        |                  N/A |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                              |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\r\n",
      "|        ID   ID                                                               Usage      |\r\n",
      "|=========================================================================================|\r\n",
      "|  No running processes found                                                             |\r\n",
      "+-----------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c779eab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:33:05.741246Z",
     "iopub.status.busy": "2025-11-03T23:33:05.740745Z",
     "iopub.status.idle": "2025-11-03T23:35:13.755180Z",
     "shell.execute_reply": "2025-11-03T23:35:13.754179Z"
    },
    "id": "fKgCL4P0LpSE",
    "papermill": {
     "duration": 128.018952,
     "end_time": "2025-11-03T23:35:13.756693",
     "exception": false,
     "start_time": "2025-11-03T23:33:05.737741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu121\r\n",
      "Collecting torch==2.5.1+cu121\r\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp311-cp311-linux_x86_64.whl (780.5 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.5/780.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1+cu121) (3.19.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1+cu121) (4.15.0)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1+cu121) (3.5)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1+cu121) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1+cu121) (2025.9.0)\r\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.5.1+cu121)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.5.1+cu121)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.5.1+cu121)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m111.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.5.1+cu121)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.5.1+cu121)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.5.1+cu121)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.5.1+cu121)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.5.1+cu121)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.5.1+cu121)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1+cu121) (2.21.5)\r\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.5.1+cu121)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting triton==3.1.0 (from torch==2.5.1+cu121)\r\n",
      "  Downloading https://download.pytorch.org/whl/triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1+cu121) (1.13.1)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.5.1+cu121) (12.5.82)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.5.1+cu121) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.5.1+cu121) (3.0.2)\r\n",
      "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch\r\n",
      "  Attempting uninstall: triton\r\n",
      "    Found existing installation: triton 3.2.0\r\n",
      "    Uninstalling triton-3.2.0:\r\n",
      "      Successfully uninstalled triton-3.2.0\r\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\r\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.4.127\r\n",
      "    Uninstalling nvidia-nvtx-cu12-12.4.127:\r\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\r\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\r\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\r\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\r\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\r\n",
      "  Attempting uninstall: nvidia-curand-cu12\r\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\r\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\r\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\r\n",
      "  Attempting uninstall: nvidia-cufft-cu12\r\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\r\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\r\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\r\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\r\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\r\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\r\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cublas-cu12\r\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\r\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\r\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\r\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\r\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\r\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\r\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\r\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\r\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\r\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\r\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\r\n",
      "  Attempting uninstall: torch\r\n",
      "    Found existing installation: torch 2.6.0+cu124\r\n",
      "    Uninstalling torch-2.6.0+cu124:\r\n",
      "      Successfully uninstalled torch-2.6.0+cu124\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\r\n",
      "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.5.1+cu121 which is incompatible.\r\n",
      "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.5.1+cu121 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nvtx-cu12-12.1.105 torch-2.5.1+cu121 triton-3.1.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.5.1+cu121 --extra-index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cfd653e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-11-03T23:35:13.820025Z",
     "iopub.status.busy": "2025-11-03T23:35:13.819492Z",
     "iopub.status.idle": "2025-11-03T23:35:15.683284Z",
     "shell.execute_reply": "2025-11-03T23:35:15.682428Z"
    },
    "id": "umRNr2AmLpSJ",
    "outputId": "7c2853de-08ba-405e-eba1-5ccb74b2123a",
    "papermill": {
     "duration": 1.896751,
     "end_time": "2025-11-03T23:35:15.684602",
     "exception": false,
     "start_time": "2025-11-03T23:35:13.787851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea89ef5",
   "metadata": {
    "id": "xtbD-iZLb5Yy",
    "papermill": {
     "duration": 0.030154,
     "end_time": "2025-11-03T23:35:15.745878",
     "exception": false,
     "start_time": "2025-11-03T23:35:15.715724",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Loss function mentioned as footnote in the paper:\n",
    "$$\n",
    "L_\\text{total}=\\displaystyle-\\frac{1}{M}\\sum_{i=1}^M\\log\\frac{\\sum_{d_r\\in R_i}\\exp(\\text{sim}(q_i, d_r)/\\tau)}{\\sum_{d_k\\in D}\\exp(\\text{sim}(q_i, d_k)/\\tau)}\n",
    "$$\n",
    "where\n",
    "- $R_i = \\{d_r \\in D \\mid d_r \\text{ relevan to query } q_i\\}$\n",
    "- $D$ set of documents\n",
    "- $k:=2$ - for simplicity\n",
    "\n",
    "The loss function used in [Google's implementation](https://github.com/google-deepmind/limit/blob/main/code/free_embedding_experiment.py):\n",
    "$$\n",
    "L_\\text{total}=\\displaystyle-\\frac{1}{M}\\sum_{i=1}^m\\sum_{d_r\\in R_i}\\log\\frac{\\exp(\\text{sim}(q_i, d_r)/\\tau)}{\\sum_{d_k\\in D}\\exp(\\text{sim}(q_i, d_k)/\\tau)}\n",
    "$$\n",
    "\n",
    "and it turned out, from the source code, that $M$ is the number of positive query-doc pairs (number of ones from the qrel matrix)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5db9543",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:35:15.807399Z",
     "iopub.status.busy": "2025-11-03T23:35:15.807069Z",
     "iopub.status.idle": "2025-11-03T23:35:15.817134Z",
     "shell.execute_reply": "2025-11-03T23:35:15.816604Z"
    },
    "id": "mSv7b50rLpSK",
    "papermill": {
     "duration": 0.042039,
     "end_time": "2025-11-03T23:35:15.818169",
     "exception": false,
     "start_time": "2025-11-03T23:35:15.776130",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import torch\n",
    "\n",
    "class FreeEmbeddingsModel(torch.nn.Module):\n",
    "    def __init__(self, num_of_docs: int, dimension: int, k: int = 2, temp: float = 0.07):\n",
    "        super().__init__()\n",
    "\n",
    "        self.__n = num_of_docs\n",
    "        self.__d = dimension\n",
    "        self.__k = k\n",
    "        self.__temp = temp\n",
    "        self.__m = None\n",
    "        self.docs = None\n",
    "        self.queries = None\n",
    "        self.__qrel_matrix = None\n",
    "\n",
    "        self.__qrel_matrix = self.__generate_qrel_matrix(self.__n, self.__k)\n",
    "        self.__m = self.__qrel_matrix.shape[0]\n",
    "\n",
    "        self.docs = torch.nn.Parameter(torch.randn(self.__n, self.__d))\n",
    "        self.queries = torch.nn.Parameter(torch.randn(self.__m, self.__d))\n",
    "        with torch.no_grad():\n",
    "            self.queries.div_(self.queries.norm(dim=1, keepdim=True))\n",
    "            self.docs.div_(self.docs.norm(dim=1, keepdim=True))\n",
    "\n",
    "    @staticmethod\n",
    "    def __generate_qrel_matrix(n: int, k: int) -> torch.Tensor:\n",
    "        combos = list(combinations(range(n), k))\n",
    "        matrix = torch.zeros((len(combos), n), dtype=torch.int)\n",
    "        for i, combo in enumerate(combos):\n",
    "            matrix[i, list(combo)] = 1\n",
    "        return matrix\n",
    "\n",
    "    # the loss function from the paper\n",
    "    # it results the linear d-critical_n curve\n",
    "    # def forward(self):\n",
    "    #     self.__qrel_matrix = self.__qrel_matrix.to(self.docs.device)\n",
    "\n",
    "    #     # normalize the vectors\n",
    "    #     docs_norm = self.docs / self.docs.norm(dim=1, keepdim=True)\n",
    "    #     queries_norm = self.queries / self.queries.norm(dim=1, keepdim=True)\n",
    "\n",
    "    #     sim = queries_norm @ docs_norm.T\n",
    "    #     print(f\"sim.mean()={sim.mean()}, sim.std()={sim.std()}\")\n",
    "    #     exp_sim = torch.exp(sim / self.__temp)\n",
    "\n",
    "    #     # filter the relevant docs using the qrel matrix as mask\n",
    "    #     num = (exp_sim * self.__qrel_matrix).sum(dim=1)\n",
    "    #     den = exp_sim.sum(dim=1)\n",
    "    #     M = self.__qrel_matrix.sum()\n",
    "    #     total_loss = -torch.log(num / (den + 1e-12)).sum() / M\n",
    "    #     return total_loss\n",
    "\n",
    "    # the loss function from their implementation\n",
    "    # it results a cubic d-critical_n curve, which is above the expected one\n",
    "    def forward(self):\n",
    "        self.__qrel_matrix = self.__qrel_matrix.to(self.docs.device)\n",
    "\n",
    "        queries_norm = self.queries #/ (self.queries.norm(dim=1, keepdim=True))\n",
    "        docs_norm = self.docs #/ (self.docs.norm(dim=1, keepdim=True))\n",
    "\n",
    "        logits = (queries_norm @ docs_norm.T) / self.__temp\n",
    "        log_probs = torch.log_softmax(logits, dim=1)\n",
    "\n",
    "        sum_pos_log_probs = (log_probs * self.__qrel_matrix).sum()\n",
    "        M = self.__qrel_matrix.sum()\n",
    "        total_loss = -sum_pos_log_probs / M\n",
    "        return total_loss\n",
    "\n",
    "    def accuracy(self) -> float:\n",
    "        docs_norm = self.docs / self.docs.norm(dim=1, keepdim=True)\n",
    "        queries_norm = self.queries / self.queries.norm(dim=1, keepdim=True)\n",
    "\n",
    "        sim = queries_norm @ docs_norm.T\n",
    "        similar_rows = 0\n",
    "        for i in range(self.__m):\n",
    "            # use masking to avoid ties\n",
    "            top_k_mask = self.__qrel_matrix[i].bool()\n",
    "\n",
    "            pos_vals = sim[i][top_k_mask]\n",
    "            neg_vals = sim[i][~top_k_mask]\n",
    "\n",
    "            if pos_vals.numel() == 0 or neg_vals.numel() == 0:\n",
    "                continue\n",
    "\n",
    "            similar_rows += int(torch.min(pos_vals) >= torch.max(neg_vals))\n",
    "\n",
    "        return similar_rows / self.__m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4ad8f43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:35:15.879844Z",
     "iopub.status.busy": "2025-11-03T23:35:15.879619Z",
     "iopub.status.idle": "2025-11-03T23:35:15.889713Z",
     "shell.execute_reply": "2025-11-03T23:35:15.888895Z"
    },
    "id": "qtR3q_E9LpSL",
    "papermill": {
     "duration": 0.041934,
     "end_time": "2025-11-03T23:35:15.890722",
     "exception": true,
     "start_time": "2025-11-03T23:35:15.848788",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19/3841251777.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# params from the original experiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m DEFAULT_EXPERIMENT_PARAMS: Dict[str, Any] = {\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"q\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"learning_rate\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"num_iterations\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dict' is not defined"
     ]
    }
   ],
   "source": [
    "# params from the original experiment\n",
    "DEFAULT_EXPERIMENT_PARAMS: Dict[str, Any] = {\n",
    "    \"q\": None,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"num_iterations\": 100000,\n",
    "    \"temperature\": 0.1,\n",
    "    \"seed\": 42,\n",
    "    \"show_progress\": True,\n",
    "    \"device\": \"gpu\",\n",
    "    \"log_interval\": 50,\n",
    "    \"early_stopping_patience\": 1000,\n",
    "    \"early_stopping_min_delta\": 0.00001,\n",
    "    \"early_stopping_monitor_metric\": \"loss\",\n",
    "    \"early_stopping_restore_best_weights\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfa22cd",
   "metadata": {
    "id": "ZUQDjmwwLpSM",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def train(\n",
    "    num_of_docs: int,\n",
    "    dimension: int,\n",
    "    max_patience: int = 1000,\n",
    "    temp: float = 0.1,\n",
    "    learning_rate: float = 0.01,\n",
    "    max_iters: int = 100000,\n",
    "    min_delta: float = 0.00001\n",
    ") -> float:\n",
    "    min_loss = torch.finfo(torch.float32).max\n",
    "    max_acc = -1\n",
    "    best_query_weights = None\n",
    "    best_doc_weights = None\n",
    "\n",
    "    device = torch.device(\"cuda\")\n",
    "    model = FreeEmbeddingsModel(num_of_docs=num_of_docs, dimension=dimension, temp=temp).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    prev_loss = torch.finfo(torch.float32).max\n",
    "    iters = 0\n",
    "    patience = 0\n",
    "    while max_patience > patience and iters < max_iters:\n",
    "        optimizer.zero_grad()\n",
    "        loss = model()\n",
    "\n",
    "        if min_loss - loss > min_delta:\n",
    "            min_loss = loss\n",
    "            best_query_weights = model.queries.detach().clone()\n",
    "            best_doc_weights = model.docs.detach().clone()\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "\n",
    "        if iters % 1000 == 0:\n",
    "            accuracy = model.accuracy()\n",
    "            print(f\"[docs={num_of_docs}, dim={dimension}]: epoch #{iters}, patience: {patience}/{max_patience}, accuracy={accuracy}, loss={loss}\")\n",
    "            if accuracy >= 1.0:\n",
    "                return 1.0\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.queries.div_(model.queries.norm(dim=1, keepdim=True))\n",
    "            model.docs.div_(model.docs.norm(dim=1, keepdim=True))\n",
    "\n",
    "        iters += 1\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.queries.copy_(best_query_weights)\n",
    "        model.docs.copy_(best_doc_weights)\n",
    "\n",
    "    return model.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc8b7a3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Opb6oGT1LpSO",
    "outputId": "be0850d4-c22f-4793-ced5-1f8132172838",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred = lambda d: -10.5322 + 4.0309*d + 0.0520*d**2 + 0.0037*d**3\n",
    "pred(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d097f4",
   "metadata": {
    "id": "yTthnBSfLpSS",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def find_critical_num_of_docs(dimension: int) -> int:\n",
    "    is_accurate = dict()\n",
    "\n",
    "    product = 5\n",
    "    while train(num_of_docs=int(product), dimension=dimension) >= 1:\n",
    "        product *= 1.5\n",
    "\n",
    "    lower = int(product / 1.5)\n",
    "    upper = int(product)\n",
    "\n",
    "    while lower <= upper:\n",
    "        middle = (lower + upper) // 2\n",
    "\n",
    "        is_mid_acc = is_accurate.get(middle)\n",
    "        if is_mid_acc is None:\n",
    "            is_mid_acc = train(num_of_docs=middle, dimension=dimension) >= 1\n",
    "            is_accurate[middle] = is_mid_acc\n",
    "\n",
    "        if not is_mid_acc and middle >= 1:\n",
    "            is_prev_mid_acc = is_accurate.get(middle - 1)\n",
    "            if is_prev_mid_acc is None:\n",
    "                is_prev_mid_acc = train(num_of_docs=middle - 1, dimension=dimension) >= 1\n",
    "                is_accurate[middle - 1] = is_prev_mid_acc\n",
    "\n",
    "            if is_prev_mid_acc:\n",
    "                return middle\n",
    "\n",
    "        if is_mid_acc:\n",
    "            lower = middle + 1\n",
    "        else:\n",
    "            upper = middle - 1\n",
    "\n",
    "    return lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a77407",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PTRc_4FuLpST",
    "outputId": "32e99b34-cf57-4c95-98ae-a493c0baf8bb",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "critical_n = []\n",
    "for d in range(9, 31):\n",
    "    critical_n.append(find_critical_num_of_docs(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06cb5a2",
   "metadata": {
    "id": "hZb-PGnMLpSU",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "critical_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafa4eb0",
   "metadata": {
    "id": "bX5tlGNNzVbl",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 134.527936,
   "end_time": "2025-11-03T23:35:16.538805",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-03T23:33:02.010869",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
