{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13859669,"sourceType":"datasetVersion","datasetId":8829386},{"sourceId":13862330,"sourceType":"datasetVersion","datasetId":8831427},{"sourceId":13869877,"sourceType":"datasetVersion","datasetId":8837077}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## LIMIT dataset construction","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# we use the originally generated attributes using gemini-pro-2.5 by the creators of the LIMIT dataset\nliked_items = pd.read_csv(\"/kaggle/input/generated-attributes/generated_attributes.csv\")\nliked_items = liked_items[\"liked_item\"].to_list()\n\n# Random first and last names, originally used by the creators of the LIMIT dataset\n# array of first names dataset, source: https://gist.github.com/ruanbekker/a1506f06aa1df06c5a9501cb393626ea#file-array-names-py\nfirst_names = pd.read_csv(\"/kaggle/input/random-names/first_names.csv\")\nfirst_names = first_names[\"first_name\"].to_list()\n# most common last name dataset, source: https://gist.github.com/craigh411/19a4479b289ae6c3f6edb95152214efc\nlast_names = pd.read_csv(\"/kaggle/input/random-names/last_names.csv\")\nlast_names = last_names[\"last_name\"].to_list()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T22:02:29.482832Z","iopub.execute_input":"2025-11-25T22:02:29.483323Z","iopub.status.idle":"2025-11-25T22:02:29.546262Z","shell.execute_reply.started":"2025-11-25T22:02:29.483292Z","shell.execute_reply":"2025-11-25T22:02:29.545148Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from itertools import combinations\nimport random\n\ndef generate_dataset(\n    liked_items: list[str] = liked_items,\n    first_names: list[str] = first_names,\n    last_names: list[str] = last_names,\n    num_of_docs: int = 46,\n    max_num_of_docs: int = 50000,\n    num_of_queries: int = 1000,\n    limit: int = 46,\n    top_k: int = 2\n):\n    # create relevant names, docs, ground-truths\n    names = [\n        f\"{random.choice(first_names)} {random.choice(last_names)}\"\n        for _ in range(num_of_docs)    \n    ]\n\n    docs = {\n        name: {\n            \"_id\": name,\n            \"liked_items\": []\n        } \n        for name in names    \n    }\n\n    selected_items = random.sample(liked_items, num_of_queries)\n    queries = []\n    qrels = []\n    for index, (comb_of_names, item) in enumerate(zip(combinations(names, top_k), selected_items)):\n        queries.append({\"_id\": f\"query_{index}\", \"text\": f\"Who likes {item}?\"})\n        for name in comb_of_names:\n            qrels.append({\"query-id\": f\"query_{index}\", \"corpus-id\": name, \"score\": 1})\n            docs[name][\"liked_items\"].append(item)\n\n    remaining_items = list(set(liked_items) - set(selected_items))\n    for name in docs:\n        docs[name][\"liked_items\"] += random.sample(remaining_items, limit - len(docs[name][\"liked_items\"]))\n\n    # create other irelevant documents\n    names = set(names)\n    for _ in range(max_num_of_docs - num_of_docs):\n        while True:\n            name = f\"{random.choice(first_names)} {random.choice(last_names)}\"\n            if name not in names:\n                names.add(name)\n                break\n        \n        docs[name] = {\n            \"_id\": name,\n            \"liked_items\": random.sample(remaining_items, limit)\n        }\n    \n    docs = pd.DataFrame([\n        {\n            \"_id\": doc[\"_id\"],\n            \"title\": \"\", \n            \"text\": f\"{doc['_id']} likes {', '.join(doc['liked_items'])}.\"\n        }\n        for doc in docs.values()\n    ])\n    docs.to_json(\"corpus.jsonl\", orient=\"records\", lines=True)\n    \n    queries = pd.DataFrame(queries)\n    queries.to_json(\"queries.jsonl\", orient=\"records\", lines=True)\n\n    qrels = pd.DataFrame(qrels)\n    qrels.to_json(\"qrels.jsonl\", orient=\"records\", lines=True)\n    \n    \ngenerate_dataset()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T22:02:33.032566Z","iopub.execute_input":"2025-11-25T22:02:33.032888Z","iopub.status.idle":"2025-11-25T22:02:34.936000Z","shell.execute_reply.started":"2025-11-25T22:02:33.032860Z","shell.execute_reply":"2025-11-25T22:02:34.934896Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## LIMIT vs other datasets","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom itertools import combinations\n\ndef query_graph_metrics(qrels: pd.DataFrame):    \n    # Get relevant documents per query\n    relevant_df = qrels[qrels['score'] > 0]\n    query_to_docs = relevant_df.groupby('query-id')['corpus-id'].apply(set).to_dict()\n    \n    query_ids = list(query_to_docs.keys())\n    vertex_count = len(query_ids)\n    \n    strength_of_query = {\n        q_id: 0\n        for q_id in query_ids\n    }\n    \n    # Add edges between queries that share relevant documents\n    edge_count = 0\n    for q1_id, q2_id in combinations(query_ids, 2):\n        docs1 = query_to_docs[q1_id]\n        docs2 = query_to_docs[q2_id]\n        intersection = len(docs1.intersection(docs2))\n        \n        if intersection > 0:\n            # Calculate Jaccard similarity\n            union = len(docs1.union(docs2))\n            jaccard_score = intersection / union if union > 0 else 0\n            if jaccard_score > 0:\n                strength_of_query[q1_id] += jaccard_score\n                strength_of_query[q2_id] += jaccard_score\n                edge_count += 1\n    \n    # Calculate graph density\n    density = 2 * edge_count / (vertex_count * (vertex_count - 1)) if vertex_count > 0 else 0\n    \n    # Calculate average edge weight (query similarity)\n    if edge_count > 0:\n        strengths = strength_of_query.values()\n        avg_strength = sum(strengths) / vertex_count if strengths else 0\n    else:\n        avg_strength = 0\n    \n    # print(f\"\\nQuery Graph Statistics:\")\n    # print(f\"  Number of queries (nodes): {vertex_count}\")\n    # print(f\"  Number of query pairs with shared docs (edges): {edge_count}\")\n    # print(f\"  Maximum possible edges: {vertex_count * (vertex_count - 1) // 2}\")\n    # print(f\"  Query Graph Density: {density:.6f}\")\n    # print(f\"  Average Query Similarity (weighted degree): {avg_strength:.4f}\")\n    \n    return density, avg_strength\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T18:23:03.461257Z","iopub.execute_input":"2025-12-01T18:23:03.461570Z","iopub.status.idle":"2025-12-01T18:23:05.316792Z","shell.execute_reply.started":"2025-12-01T18:23:03.461546Z","shell.execute_reply":"2025-12-01T18:23:05.313923Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from datasets import load_dataset\n\nqrels = pd.read_json(\"hf://datasets/orionweller/LIMIT/qrels.jsonl\", lines=True)\nprint(\"LIMIT\", query_graph_metrics(qrels))\n\nqrels = pd.read_parquet(\"hf://datasets/mteb/Core17InstructionRetrieval/qrels/test-00000-of-00001.parquet\")\nprint(\"core 17 ir\", query_graph_metrics(qrels))\n\nqrels = pd.read_json(\"hf://datasets/mteb/hotpotqa/qrels/test.jsonl\", lines=True)\nprint(\"hotpotqa\", query_graph_metrics(qrels))\n\nqrels = pd.read_json(\"hf://datasets/mteb/scifact/qrels/test.jsonl\", lines=True)\nprint(\"scifact\", query_graph_metrics(qrels))\n\nqrels = pd.read_json(\"hf://datasets/mteb/nq/qrels/test.jsonl\", lines=True)\nprint(\"nq\", query_graph_metrics(qrels))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T18:23:45.541519Z","iopub.execute_input":"2025-12-01T18:23:45.542367Z","iopub.status.idle":"2025-12-01T18:24:07.276806Z","shell.execute_reply.started":"2025-12-01T18:23:45.542315Z","shell.execute_reply":"2025-12-01T18:24:07.275649Z"}},"outputs":[{"name":"stdout","text":"LIMIT (0.08548148148148148, 28.4653333333333)\ncore 17 ir (0.02564102564102564, 0.5911717092249338)\nhotpotqa (3.735411739771666e-05, 0.11037587215845267)\nscifact (0.0014492753623188406, 0.4222222222222222)\nnq (0.0, 0)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"custom_corpus = load_dataset(\"json\", data_files=\"/kaggle/input/custom-limit/corpus.jsonl\", split=\"all\")\ncustom_corpus = custom_corpus.to_pandas()\ncustom_qrels  = load_dataset(\"json\", data_files=\"/kaggle/input/custom-limit/qrels.jsonl\", split=\"all\")\ncustom_qrels = custom_qrels.to_pandas()\ncustom_queries = load_dataset(\"json\", data_files=\"/kaggle/input/custom-limit/queries.jsonl\", split=\"all\")\ncustom_queries = custom_queries.to_pandas()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T18:24:11.173187Z","iopub.execute_input":"2025-12-01T18:24:11.174214Z","iopub.status.idle":"2025-12-01T18:24:12.299381Z","shell.execute_reply.started":"2025-12-01T18:24:11.174177Z","shell.execute_reply":"2025-12-01T18:24:12.297841Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2738a1fcf63c41209b87257c93379039"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"572afeb479174a2da855ec3097073e4f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed3d6709aa0447fdb878ee35192dd6dd"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"print(\"custom_limit\", query_graph_metrics(custom_qrels))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T18:24:14.550062Z","iopub.execute_input":"2025-12-01T18:24:14.550791Z","iopub.status.idle":"2025-12-01T18:24:14.732048Z","shell.execute_reply.started":"2025-12-01T18:24:14.550762Z","shell.execute_reply":"2025-12-01T18:24:14.731052Z"}},"outputs":[{"name":"stdout","text":"custom_limit (0.08548148148148148, 28.4653333333333)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}